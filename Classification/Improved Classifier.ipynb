{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:08:20.219728Z",
     "start_time": "2023-06-03T10:08:20.194803Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from yaspin import yaspin\n",
    "from yaspin.spinners import Spinners\n",
    "import ast\n",
    "import swifter\n",
    "from datetime import datetime\n",
    "from socialvec.socialvec import SocialVec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from aux_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:07:07.049907Z",
     "start_time": "2023-06-03T10:07:07.033942Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = \"config.yaml\"\n",
    "\n",
    "with open(config, 'r') as file:\n",
    "    conf = yaml.load(file, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:02:08.969091Z",
     "start_time": "2023-06-03T10:02:06.275989Z"
    }
   },
   "outputs": [],
   "source": [
    "# read and arrange data\n",
    "with yaspin(Spinners.arc, text=\"Reading Data\") as sp:\n",
    "    try:\n",
    "        data_for_training = pd.read_csv(conf['data_file'])\n",
    "    except:\n",
    "        data_for_training = pd.read_csv(conf['data_file'], compression='gzip')\n",
    "    data_for_training.reset_index(inplace=True)\n",
    "    data_for_training.drop('index', axis=1, inplace=True)\n",
    "    data_for_training.drop(data_for_training.columns[data_for_training.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "\n",
    "    data_for_training = data_for_training[data_for_training['source']==conf[\"source_for_modeling\"]]\n",
    "    data_for_training = data_for_training[~data_for_training['list'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:03:07.860064Z",
     "start_time": "2023-06-03T10:02:08.970675Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Parsing Lists\")\n",
    "data_for_training['list'] = data_for_training.progress_apply(lambda x: fix_list(x), axis=1)\n",
    "data_for_training.list = data_for_training.list.swifter.apply(lambda x: list(ast.literal_eval(x.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:03:54.883147Z",
     "start_time": "2023-06-03T10:03:07.865447Z"
    }
   },
   "outputs": [],
   "source": [
    "if conf[\"use_existing_train_test_split\"] == True:\n",
    "\n",
    "    print(\"Using existing train/test split ✅✅\")\n",
    "    df_test_set = pd.read_excel(conf['train_test_split_file'])\n",
    "    data_for_training = data_for_training.merge(df_test_set, on='twitter_id', how='left')\n",
    "else:\n",
    "\n",
    "    print(\"Creating a new train/test split ‼️‼️\")\n",
    "    # Split the data into train and test sets, stratified by a specific column\n",
    "    train_df, test_df = train_test_split(data_for_training,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=data_for_training[conf['field_to_classify']])\n",
    "\n",
    "    # Create a new column 'dataset' and initialize with 'train' for all rows\n",
    "    data_for_training.loc[data_for_training.index.isin(train_df.index), 'train_test'] = 'train'\n",
    "\n",
    "    # Use the loc accessor to update the 'dataset' column for the test set rows\n",
    "    data_for_training.loc[data_for_training.index.isin(test_df.index), 'train_test'] = 'test'\n",
    "\n",
    "    date_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "    data_for_training[['twitter_id','train_test']].to_excel(f'../data/{conf[\"field_to_classify\"]}_train_test_split_{date_string}.xlsx', index=False)\n",
    "\n",
    "\n",
    "sv = SocialVec(conf[\"SocialVec_version\"])\n",
    "data_for_training[['socialvec', 'socialvec_len']] = data_for_training.progress_apply(lambda x: sv.get_average_embeddings(x['list']), axis=1, result_type='expand' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:03:54.887894Z",
     "start_time": "2023-06-03T10:03:54.884808Z"
    }
   },
   "outputs": [],
   "source": [
    "#data_for_training.drop('political_train_test', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:05:31.691698Z",
     "start_time": "2023-06-03T10:05:31.668478Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data_for_training['class'] = le.fit_transform(data_for_training[conf['field_to_classify']])\n",
    "data_for_training = data_for_training[data_for_training['socialvec_len']>conf['minimal_socialvec_len']]\n",
    "print (f\"number of samples after filtering by SocialVec len: {data_for_training.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:05:33.646960Z",
     "start_time": "2023-06-03T10:05:33.629130Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = data_for_training[data_for_training['train_test']=='train'].copy()\n",
    "test_df = data_for_training[data_for_training['train_test']=='test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:05:36.324367Z",
     "start_time": "2023-06-03T10:05:36.160889Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train     = prep_tf_inputs(train_df, 'socialvec')\n",
    "X_test,  y_test      = prep_tf_inputs(test_df, 'socialvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:05:37.742922Z",
     "start_time": "2023-06-03T10:05:37.731200Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_smoothing_loss(y_true, y_pred, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Custom loss function implementing label smoothing.\n",
    "    \"\"\"\n",
    "    num_classes = y_true.shape[-1]\n",
    "    smooth_positives = 1.0 - smoothing\n",
    "    smooth_negatives = smoothing / num_classes\n",
    "    y_true = y_true * smooth_positives + smooth_negatives\n",
    "\n",
    "    return keras.losses.categorical_crossentropy(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:44:35.351865Z",
     "start_time": "2023-06-03T06:44:35.240190Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input_shape = (X_train.shape[1],)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add a Dense layer with 64 units and ReLU activation\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n",
    "\n",
    "# Add a Dense layer with 2 output neurons (representing the two classes) and softmax activation\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "                loss= label_smoothing_loss, #'categorical_crossentropy',\n",
    "                metrics=[Precision()],\n",
    "                run_eagerly=True)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:44:42.589494Z",
     "start_time": "2023-06-03T06:44:42.570473Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-dimensional array containing zeros and ones\n",
    "data = y_train\n",
    "\n",
    "# Reshape the data to a 2D array with a single feature\n",
    "data_2d = data.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "one_hot_encoded = tf.constant(encoder.fit_transform(data_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:04:53.677242Z",
     "start_time": "2023-06-03T06:45:03.899973Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "              one_hot_encoded,\n",
    "              epochs=100,\n",
    "              batch_size=50,\n",
    "              validation_split=0.1,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:08:34.874190Z",
     "start_time": "2023-06-03T10:08:34.769841Z"
    }
   },
   "outputs": [],
   "source": [
    "if conf['load_model']:\n",
    "\n",
    "    custom_objects = {'label_smoothing_loss': label_smoothing_loss}\n",
    "    with keras.utils.custom_object_scope(custom_objects):    \n",
    "        model = keras.models.load_model(f\"models/{conf['model_name']}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:08:37.828312Z",
     "start_time": "2023-06-03T10:08:37.482140Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_full = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:08:41.125750Z",
     "start_time": "2023-06-03T10:08:40.943783Z"
    }
   },
   "outputs": [],
   "source": [
    "test_preds = tf.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:08:41.863753Z",
     "start_time": "2023-06-03T10:08:41.841943Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:09:30.218719Z",
     "start_time": "2023-06-03T10:09:30.208592Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['preds'] = test_preds\n",
    "test_df['confidence'] = np.max((preds_full), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:09:32.148073Z",
     "start_time": "2023-06-03T10:09:31.578993Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# y_test  : actual labels or target\n",
    "# y_preds : predicted labels or target\n",
    "sns.heatmap(confusion_matrix(y_test, test_preds),square=True, annot=True, cmap='Blues', fmt='d', cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High confidence results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:11:18.752797Z",
     "start_time": "2023-06-03T10:11:18.742459Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:11:43.465046Z",
     "start_time": "2023-06-03T10:11:43.454539Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df_high_confidence = test_df[test_df['confidence']>0.9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:11:44.739141Z",
     "start_time": "2023-06-03T10:11:44.718516Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_df_high_confidence['class'], test_df_high_confidence['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:12:18.506729Z",
     "start_time": "2023-06-03T10:12:18.401824Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(test_df_high_confidence['class'], test_df_high_confidence['preds']),square=True, annot=True, cmap='Blues', fmt='d', cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:14:51.990383Z",
     "start_time": "2023-06-03T07:14:51.944872Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save(f\"models/{conf[\"model_name\"]}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the wrong predictions for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:15:26.652365Z",
     "start_time": "2023-06-03T07:15:19.757266Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df_debug = test_df[['twitter_id', 'list', conf[\"field_to_classify\"], 'source', 'train_test',\n",
    "       'socialvec_len', 'class', 'preds', 'confidence']].copy()\n",
    "\n",
    "test_df_debug.to_csv(f\"{conf['field_to_classify']}_test_df_debug.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T09:52:13.448802Z",
     "start_time": "2023-06-03T09:52:13.437737Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
